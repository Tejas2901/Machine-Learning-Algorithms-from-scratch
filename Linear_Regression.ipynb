{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear_Regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyObj8HcCxdXOQPFMCD9/Pk+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Step 1: Importing basic libraries"],"metadata":{"id":"k9EbmcVXRBp8"}},{"cell_type":"code","source":["from IPython.display import Math, Latex , display\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","%matplotlib inline"],"metadata":{"id":"7wFV6snyRGrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 2: Combine linear regression components"],"metadata":{"id":"2_eKYM6-Rou1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2afTjDxmQecT"},"outputs":[],"source":["class LinReg(object):\n","  '''Linear Regression\n","  ----------------------\n","  X: feature matrix\n","  y: label vector\n","  w: weight vector\n","  y = X@w\n","  '''\n","\n","  def __init__(self):\n","    self.to = 200\n","    self.t1 = 1000\n","\n","  def predict(self,X:np.ndarray):           \n","\n","    y = X @ self.w\n","    return y\n","\n","  def loss(self,X:np.ndarray,y:np.array):\n","\n","    e = y - self.predict(X)  \n","    return (1/2)*(np.transpose(e) @ e)\n","\n","\n","\n","  def rmse(self,X:np.ndarray , y:np.array):\n","    return np.sqrt((2/X.shape[0])*(self.loss(X,y)))\n","\n","  def fit(self,X:np.ndarray, y:np.array):\n","    self.w =  np.linalg.pinv(X) @ y\n","    return self.w\n","\n","\n","\n","  def calculate_gradient(self,X:np.ndarray , y:np.array):\n","    return np.transpose(X) @ (self.predict(X)-y)\n","\n","  def update_weights(self, grad:np.ndarray , lr:float):\n","    return (self.w - lr*grad)\n","\n","  def learning_schedule(self,t):\n","    return self.to/(t + self.t1)\n","\n","  def gd(self,X:np.ndarray , y:np.array , num_epochs:int, lr:float):\n","    '''num_epochs = number of iterations\n","      lr: learning rate'''\n","    self.w = np.zeros((X.shape[1])) #intialize the weight vector as zero vector\n","    self.w_all = []\n","    self.err_all  = []\n","    for i in np.range(0,num_epochs):\n","      dJdW = self.calculate_gradient(X, y)\n","      self.w_all.append(self.w)\n","      self.err_all.append(self.loss(X,y))\n","      self.w = self.update_weights(dJdW, lr)\n","    return self.w\n","\n","  def mini_batch_gd(self,X:np.ndarray,y:np.ndarray, num_iters:int,minibatch_size:int):\n","    '''\n","    Estimates parameters of linear regression model through gradeint descent\n","\n","    Args:\n","      1: X: Feature matrix for training data\n","      2: y: Label vector for training data\n","      3: num_iters : Number of iterations\n","    '''\n","    w_all =[] # All parameters across iterations\n","    err_all =[] # Error across itertations\n","\n","    # Parameter vector initialized to [0,0]\n","\n","    self.w = np.zeros((X.shape[1]))\n","    self.t = 0\n","\n","    for epoch in range(num_iters):\n","      shuffled_indices = np.random.permutation(X.shape[0])\n","      X_shuffled = X[shuffled_indices]\n","      y_shuffled = y[shuffled_indices]\n","\n","      for i in range(0,X.shape[0], minibatch_size):\n","        self.t += 1\n","        xi = X_shuffled[i:i+minibatch_size]\n","        yi = y_shuffled[i:i+minibatch_size]\n","        err_all.append(self.loss(xi,yi,w))\n","\n","        gradients = 2/minibatch_size * self.calculate_gradient(xi,yi,w)\n","        lr = self.learning_schedule(self.t)\n","\n","        w = self.update_weights(w,gradients,lr)\n","        w_all.append(w)\n","\n","    return self.w  \n","  def sgd(self,X:np.ndarray,y:np.ndarray, num_epochs:int):\n","    '''\n","      Estimates the parameters of linear regression model through Gradient descent\n","\n","      Args:\n","          1: X: Feature matrix for training data\n","          2: y: Label matrix for training data\n","          3: num_epochs : Number of epochs\n","      Returns:\n","        Weight vector: Final weight vector\n","        Error vector across different Iterations\n","        Weight vectors across different Iterations\n","\n","    '''\n","    self.w_all = [] # all parameters across iterations\n","    self.err_all = [] # error across iterations\n","\n","    # Parameter vector initialized to [0,0]\n","\n","    self.w = np.zeros((X.shape[0]))\n","\n","    for epoch in range(num_epochs):\n","      for i in range(X.shape[0]):\n","        random_index = np.random.randint(X.shape[0])\n","        xi = X[random_index:random_index+1]\n","        yi = y[random_index:random_index+1]\n","        self.w_all.append(self.w)\n","        self.err_all.append(self.loss(xi,yi))\n","\n","        gradients = 2 * self.calculate_gradient(xi,yi)\n","        lr = self.learning_schedule(num_epochs* X.shape[0]+i)\n","\n","        self.w = self.update_weights(gradients,lr)\n","       \n","\n","    return self.w\n","\n","\n","\n","\n","\n"]}]}